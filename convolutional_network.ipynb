{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "colab_type": "code",
        "id": "YS4LtKD0-1-y",
        "outputId": "7ec3c982-1ed1-44e9-922b-49470e2ee84b"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv2\n",
        "\n",
        "# Import MINST data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# normalization\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0P0JakHS-1-2"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "learning_rate = 0.005\n",
        "training_epochs = 4\n",
        "batch_size = 512\n",
        "display_step = 10\n",
        "\n",
        "# Network Parameters\n",
        "n_input = 784 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)\n",
        "dropout = 0.85 # исключение Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])), # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])), # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])), # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes])) # 1024 inputs, 10 outputs (class prediction)\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes])) #свободные веса\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bIZURH54_o2i"
      },
      "outputs": [],
      "source": [
        "# reshape MINST data\n",
        "ds_train = x_train.reshape(x_train.shape[0], n_input)\n",
        "ds_test = x_test.reshape(x_test.shape[0], n_input)\n",
        "label_test=np.zeros(shape=(y_test.shape[0],n_classes),dtype=\"float32\")\n",
        "for k in range(y_test.shape[0]):\n",
        "  label_test[k][y_test[k]]=1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8QGQX9Js-1-7"
      },
      "outputs": [],
      "source": [
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "colab_type": "code",
        "id": "-BTQq-Ng-1_A",
        "outputId": "2d7011ae-3904-44ee-d956-86bb92ba8cc7"
      },
      "outputs": [],
      "source": [
        "# Launch the graph\n",
        "def run(learning_rate, training_epochs, batch_size, display_step, ds_test, label_test):\n",
        "   # Construct model\n",
        "    pred = conv_net(x, weights, biases, keep_prob)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "    # Evaluate model\n",
        "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    # Initializing the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        step = 1\n",
        "        batch_xs=np.zeros(shape=(batch_size,784),dtype=\"float32\")\n",
        "        \n",
        "        # Training cycle\n",
        "        for epoch in range(training_epochs):\n",
        "            batch_ys=np.zeros(shape=(batch_size,n_classes),dtype=\"float32\")\n",
        "            total_batch = int(ds_train.shape[0]/batch_size)\n",
        "        \n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_ys=np.zeros(shape=(batch_size,n_classes),dtype=\"float32\")\n",
        "            for k in range(batch_size):\n",
        "                index = k+batch_size*i\n",
        "                batch_xs[k] = ds_train[index] \n",
        "                batch_ys[k][y_train[index]]=1.0\n",
        "            \n",
        "            # Run optimization op (backprop)\n",
        "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
        "            \n",
        "            if step != 0 and step % display_step == 0:\n",
        "                # Calculate batch loss and accuracy\n",
        "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
        "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
        "                    \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                    \"{:.5f}\".format(acc))\n",
        "            step += 1\n",
        "        #print(\"Optimization Finished!\")\n",
        "        \n",
        "        # Calculate accuracy for 256 mnist test images\n",
        "        \"\"\"        print(\"Testing Accuracy:\", \\\n",
        "            sess.run(accuracy, feed_dict={x: ds_test[:256],\n",
        "                                        y: label_test[:256],\n",
        "                                        keep_prob: 1.}))\n",
        "        \"\"\"\n",
        "        return sess.run(accuracy, feed_dict={x: ds_test,\n",
        "                                        y: label_test,\n",
        "                                        keep_prob: 1.})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 5120, Minibatch Loss= 30280.326172, Training Accuracy= 0.69336\n",
            "Iter 10240, Minibatch Loss= 7466.995117, Training Accuracy= 0.73242\n",
            "Iter 15360, Minibatch Loss= 2734.855957, Training Accuracy= 0.86133\n",
            "Iter 20480, Minibatch Loss= 1790.861816, Training Accuracy= 0.91406\n",
            "Iter 25600, Minibatch Loss= 1515.815186, Training Accuracy= 0.89844\n",
            "Iter 30720, Minibatch Loss= 1226.599121, Training Accuracy= 0.89844\n",
            "Iter 35840, Minibatch Loss= 712.312256, Training Accuracy= 0.92969\n",
            "Iter 40960, Minibatch Loss= 738.901184, Training Accuracy= 0.94922\n",
            "Iter 46080, Minibatch Loss= 1177.577148, Training Accuracy= 0.89844\n",
            "Iter 51200, Minibatch Loss= 388.500000, Training Accuracy= 0.94727\n",
            "Iter 56320, Minibatch Loss= 265.766693, Training Accuracy= 0.96680\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9469"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run(learning_rate, training_epochs, batch_size, display_step, ds_test, label_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number  n0  was understood:  1.0\n",
            "Number  n1  was understood:  0.0\n",
            "Number  n2  was understood:  1.0\n",
            "Number  n3  was understood:  1.0\n",
            "Number  n4  was understood:  1.0\n",
            "Number  n5  was understood:  1.0\n",
            "Number  n6  was understood:  0.0\n",
            "Number  n7  was understood:  0.0\n",
            "Number  n8  was understood:  1.0\n",
            "Number  n9  was understood:  0.0\n"
          ]
        }
      ],
      "source": [
        "display_step = 10000\n",
        "for q in range(0, 10):\n",
        "    image = cv2.imread('nm'+str(q)+'.png', cv2.IMREAD_GRAYSCALE)\n",
        "    image = [*(image / 255.0).flatten()]\n",
        "    image = [abs(x - 1) for x in image]\n",
        "    label_test = [0] * 10\n",
        "    label_test[q] = 1\n",
        "    res = run(learning_rate, training_epochs, batch_size, display_step, [image], [label_test])\n",
        "    print('Number ', 'n'+str(q), ' was understood: ', res)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "convolutional_network (2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
